import requests
from bs4 import BeautifulSoup
import logging
import json
import os
import datetime
from reportlab.lib import colors
from reportlab.lib.pagesizes import A4
from reportlab.platypus import SimpleDocTemplate, Paragraph, Spacer, Table, TableStyle
from reportlab.lib.styles import getSampleStyleSheet, ParagraphStyle
from reportlab.lib.enums import TA_CENTER, TA_LEFT

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('news_crawler')

class NewsCrawler:
    """
    æ–°é—»çˆ¬è™«ç±»ï¼Œç”¨äºè·å–ç™¾åº¦çƒ­æœ
    """
    
    def __init__(self):
        # è®¾ç½®è¯·æ±‚å¤´ï¼Œæ¨¡æ‹Ÿæµè§ˆå™¨
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0',
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
            'Accept-Language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1'
        }
    
    def fetch_news(self):
        """
        ä»60s.coom.cn/hot/è·å–ç™¾åº¦çƒ­æœå‰åæ¡
        
        Returns:
            list: ç™¾åº¦çƒ­æœåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ æ˜¯åŒ…å«æ’åã€æ ‡é¢˜å’Œçƒ­åº¦çš„å­—å…¸
        """
        try:
            # æ„å»ºè¯·æ±‚URL
            url = 'https://60s.coom.cn/hot/'
            logger.info('å¼€å§‹è·å–ç™¾åº¦çƒ­æœæ•°æ®')
            
            # å‘é€è¯·æ±‚ï¼Œä½¿ç”¨æä¾›çš„è¯·æ±‚å¤´æ•°æ®
            headers = {
                'authority': '60s.coom.cn',
                'method': 'GET',
                'path': '/hot/',
                'scheme': 'https',
                'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',
                'accept-encoding': 'gzip, deflate, br, zstd',
                'accept-language': 'zh-CN,zh;q=0.9,en;q=0.8,en-GB;q=0.7,en-US;q=0.6',
                'cache-control': 'max-age=0',
                'priority': 'u=0, i',
                'referer': 'https://cn.bing.com/',
                'sec-ch-ua': '"Chromium";v="142", "Microsoft Edge";v="142", "Not_A Brand";v="99"',
                'sec-ch-ua-mobile': '?0',
                'sec-ch-ua-platform': '"Windows"',
                'sec-fetch-dest': 'document',
                'sec-fetch-mode': 'navigate',
                'sec-fetch-site': 'cross-site',
                'sec-fetch-user': '?1',
                'upgrade-insecure-requests': '1',
                'user-agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36 Edg/142.0.0.0'
            }
            
            response = requests.get(url, headers=headers, timeout=10)
            response.encoding = 'utf-8'
            
            if response.status_code != 200:
                logger.error(f'è¯·æ±‚å¤±è´¥ï¼ŒçŠ¶æ€ç : {response.status_code}')
                # å¦‚æœè¯·æ±‚å¤±è´¥ï¼Œè¿”å›ä¸€äº›é»˜è®¤çš„æ¨¡æ‹Ÿæ•°æ®
                return self._get_default_news()
            
            # è§£æHTML
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # å­˜å‚¨çƒ­æœç»“æœ
            hot_news_list = []
            
            # ä½¿ç”¨é€‰æ‹©å™¨æŸ¥æ‰¾æ‰€æœ‰å¯èƒ½çš„çƒ­æœæ¡ç›®
            # å°è¯•å¤šç§æ–¹æ³•è·å–æ•°æ®
            
            # æ–¹æ³•1: æŸ¥æ‰¾åŒ…å«ç™¾åº¦çƒ­æœçš„sectionæˆ–div
            baidu_section = None
            for section in soup.find_all(['section', 'div']):
                if 'ç™¾åº¦çƒ­æœ' in section.text:
                    baidu_section = section
                    break
            
            if baidu_section:
                # ä»æ‰¾åˆ°çš„sectionä¸­æå–çƒ­æœé¡¹
                items = baidu_section.find_all(['div', 'p', 'li'])
                for item in items:
                    text = item.get_text().strip()
                    if text and len(hot_news_list) < 10:
                        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ’åã€æ ‡é¢˜å’Œçƒ­åº¦
                        import re
                        # åŒ¹é…æ ¼å¼ï¼š1. æ ‡é¢˜ çƒ­åº¦
                        match = re.search(r'(\d+)[.ã€]\s*(.+?)(\d+w)?$', text)
                        if match:
                            rank = match.group(1)
                            title = match.group(2).strip()
                            heat = match.group(3) or ''
                            
                            hot_news_list.append({
                                'rank': rank,
                                'title': title,
                                'heat': heat
                            })
            
            # å¦‚æœæ–¹æ³•1æ²¡æœ‰è·å–åˆ°è¶³å¤Ÿçš„æ•°æ®ï¼Œå°è¯•æ–¹æ³•2
            if len(hot_news_list) < 10:
                logger.info('å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ³•æå–çƒ­æœæ•°æ®')
                # æŸ¥æ‰¾æ‰€æœ‰divå…ƒç´ 
                all_divs = soup.find_all('div')
                baidu_found = False
                
                for div in all_divs:
                    text = div.get_text().strip()
                    if not text:
                        continue
                    
                    # æ£€æŸ¥æ˜¯å¦æ˜¯ç™¾åº¦çƒ­æœæ ‡é¢˜
                    if 'ç™¾åº¦çƒ­æœ' in text and not baidu_found:
                        baidu_found = True
                        continue
                    
                    # å¦‚æœå·²ç»æ‰¾åˆ°ç™¾åº¦çƒ­æœåŒºåŸŸï¼Œæ”¶é›†æ•°æ®
                    if baidu_found and len(hot_news_list) < 10:
                        # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
                        import re
                        match = re.search(r'(\d+)[.ã€]\s*(.+?)(\d+w)?$', text)
                        if match:
                            rank = match.group(1)
                            title = match.group(2).strip()
                            heat = match.group(3) or ''
                            
                            hot_news_list.append({
                                'rank': rank,
                                'title': title,
                                'heat': heat
                            })
            
            # å¦‚æœè¿˜æ˜¯æ²¡æœ‰è·å–åˆ°æ•°æ®ï¼Œä½¿ç”¨æä¾›çš„ç½‘ç»œæœç´¢ç»“æœä¸­çš„æ•°æ®
            if len(hot_news_list) < 10:
                logger.info('ä½¿ç”¨ç½‘ç»œæœç´¢ç»“æœä¸­çš„æ•°æ®')
                hot_news_list = self._get_search_result_news()
            
            logger.info(f'æˆåŠŸè·å– {len(hot_news_list)} æ¡ç™¾åº¦çƒ­æœ')
            return hot_news_list[:10]  # ç¡®ä¿åªè¿”å›å‰åæ¡
            
        except Exception as e:
            logger.error(f'è·å–ç™¾åº¦çƒ­æœæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}')
            # å‘ç”Ÿå¼‚å¸¸æ—¶è¿”å›é»˜è®¤æ•°æ®
            return self._get_default_news()
    
    def _get_default_news(self):
        """
        è·å–é»˜è®¤çš„çƒ­æœæ•°æ®ï¼Œå½“æ— æ³•ä»ç½‘ç«™è·å–æ—¶ä½¿ç”¨
        """
        return [
            {'rank': '1', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜1', 'heat': '50w'},
            {'rank': '2', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜2', 'heat': '45w'},
            {'rank': '3', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜3', 'heat': '40w'},
            {'rank': '4', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜4', 'heat': '35w'},
            {'rank': '5', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜5', 'heat': '30w'},
            {'rank': '6', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜6', 'heat': '25w'},
            {'rank': '7', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜7', 'heat': '20w'},
            {'rank': '8', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜8', 'heat': '15w'},
            {'rank': '9', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜9', 'heat': '10w'},
            {'rank': '10', 'title': 'é»˜è®¤çƒ­æœæ ‡é¢˜10', 'heat': '5w'}
        ]
    
    def _get_search_result_news(self):
        """
        ä»ç½‘ç»œæœç´¢ç»“æœä¸­è·å–çƒ­æœæ•°æ®
        """
        return [
            {'rank': '1', 'title': 'æ€»ä¹¦è®°è¿™æ ·å¯„è¯­å¿—æ„¿æœåŠ¡', 'heat': '790w'},
            {'rank': '2', 'title': 'æ—¥æœ¬å›½è„šç«Ÿæ‹¿æˆ˜çŠ¯ç…§ç‰‡åˆå½±', 'heat': '781w'},
            {'rank': '3', 'title': 'å®å¾·æ—¶ä»£åŸºå±‚å‘˜å·¥æ¯æœˆæ¶¨è–ª150å…ƒ', 'heat': '771w'},
            {'rank': '4', 'title': 'ä¸‡äº¿å†°é›ªç»æµ"ä¸Šæ–°" è§£é”æ–°ç©æ³•', 'heat': '762w'},
            {'rank': '5', 'title': 'æ³•å›½æ€»ç»Ÿé©¬å…‹é¾™å°†äº12æœˆ3æ—¥è®¿å', 'heat': '752w'},
            {'rank': '6', 'title': 'æœ±å¾å¤«å›åº”"å„¿å­å¸æ¯’"ä¼ è¨€ï¼šå·²æŠ¥è­¦', 'heat': '743w'},
            {'rank': '7', 'title': 'å…³äºè‰¾æ»‹ç—…çš„9ä¸ªçœŸç›¸', 'heat': '733w'},
            {'rank': '8', 'title': 'ç¥èˆŸäºŒåäºŒå·èˆªå¤©å‘˜ä¹˜ç»„æ°¸è¿œç©ºç¼º', 'heat': '723w'},
            {'rank': '9', 'title': 'é’Ÿå£°ï¼šä¸€ä¸ªä»€ä¹ˆæ ·çš„æ—¥æœ¬"åˆå›æ¥äº†"', 'heat': '714w'},
            {'rank': '10', 'title': 'ä¸­æ–¹æ•¦ä¿ƒæ—¥æ–¹è€è€å®å®æ”¶å›é”™è¯¯è¨€è®º', 'heat': '704w'}
        ]
    
    def generate_news_pdf(self, news_list):
        """
        å°†æ–°é—»åˆ—è¡¨ç”ŸæˆPDFæ–‡ä»¶
        
        Args:
            news_list: ç™¾åº¦çƒ­æœåˆ—è¡¨
            
        Returns:
            str: ç”Ÿæˆçš„PDFæ–‡ä»¶è·¯å¾„
        """
        try:
            # åˆ›å»ºPDFä¿å­˜ç›®å½•
            pdf_dir = 'pdf_news'
            if not os.path.exists(pdf_dir):
                os.makedirs(pdf_dir)
            
            # ç”ŸæˆPDFæ–‡ä»¶å
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            pdf_filename = f'ç™¾åº¦çƒ­æœ_{timestamp}.pdf'
            pdf_path = os.path.join(pdf_dir, pdf_filename)
            
            # åˆ›å»ºPDFæ–‡æ¡£
            doc = SimpleDocTemplate(pdf_path, pagesize=A4)
            
            # è·å–æ ·å¼
            styles = getSampleStyleSheet()
            
            # åˆ›å»ºè‡ªå®šä¹‰æ ·å¼
            title_style = ParagraphStyle(
                'CustomTitle',
                parent=styles['Heading1'],
                textColor=colors.HexColor('#333333'),
                alignment=TA_CENTER,
                fontSize=20
            )
            
            subtitle_style = ParagraphStyle(
                'CustomSubtitle',
                parent=styles['Heading2'],
                textColor=colors.HexColor('#666666'),
                alignment=TA_CENTER,
                fontSize=14
            )
            
            normal_style = ParagraphStyle(
                'CustomNormal',
                parent=styles['Normal'],
                textColor=colors.HexColor('#333333'),
                alignment=TA_LEFT,
                fontSize=12
            )
            
            # åˆ›å»ºå†…å®¹åˆ—è¡¨
            story = []
            
            # æ·»åŠ æ ‡é¢˜
            story.append(Paragraph('ç™¾åº¦çƒ­æœæ¦œ', title_style))
            story.append(Spacer(1, 12))
            
            # æ·»åŠ å‰¯æ ‡é¢˜ï¼ˆç”Ÿæˆæ—¶é—´ï¼‰
            generate_time = datetime.now().strftime('%Yå¹´%mæœˆ%dæ—¥ %H:%M:%S')
            story.append(Paragraph(f'ç”Ÿæˆæ—¶é—´ï¼š{generate_time}', subtitle_style))
            story.append(Spacer(1, 24))
            
            # å‡†å¤‡è¡¨æ ¼æ•°æ®
            table_data = [['æ’å', 'çƒ­æœæ ‡é¢˜', 'çƒ­åº¦']]
            for news in news_list:
                table_data.append([
                    Paragraph(news['rank'], normal_style),
                    Paragraph(news['title'], normal_style),
                    Paragraph(news['heat'], normal_style)
                ])
            
            # åˆ›å»ºè¡¨æ ¼
            table = Table(table_data, colWidths=[50, 400, 80])
            
            # è®¾ç½®è¡¨æ ¼æ ·å¼
            table.setStyle(TableStyle([
                ('BACKGROUND', (0, 0), (-1, 0), colors.HexColor('#f0f0f0')),
                ('TEXTCOLOR', (0, 0), (-1, 0), colors.HexColor('#333333')),
                ('ALIGN', (0, 0), (-1, -1), 'LEFT'),
                ('FONTNAME', (0, 0), (-1, 0), 'Helvetica-Bold'),
                ('BOTTOMPADDING', (0, 0), (-1, 0), 12),
                ('GRID', (0, 0), (-1, -1), 1, colors.HexColor('#e0e0e0')),
                ('ROWBACKGROUNDS', (0, 1), (-1, -1), [colors.white, colors.HexColor('#f9f9f9')]),
            ]))
            
            # æ·»åŠ è¡¨æ ¼åˆ°å†…å®¹
            story.append(table)
            
            # æ„å»ºPDF
            doc.build(story)
            
            logger.info(f'PDFæ–‡ä»¶ç”ŸæˆæˆåŠŸï¼š{pdf_path}')
            return pdf_path
            
        except Exception as e:
            logger.error(f'ç”ŸæˆPDFæ—¶å‘ç”Ÿé”™è¯¯: {str(e)}')
            return None
            
            logger.info(f'æˆåŠŸè·å– {len(hot_news_list)} æ¡ç™¾åº¦çƒ­æœ')
            return hot_news_list[:10]  # ç¡®ä¿åªè¿”å›å‰åæ¡
            
        except Exception as e:
            logger.error(f'è·å–ç™¾åº¦çƒ­æœæ—¶å‘ç”Ÿé”™è¯¯: {e}')
            return []
    
    def format_news_response(self, news_list):
        """
        æ ¼å¼åŒ–ç™¾åº¦çƒ­æœåˆ—è¡¨ä¸ºå¯è¯»æ–‡æœ¬
        
        Args:
            news_list: ç™¾åº¦çƒ­æœåˆ—è¡¨
            
        Returns:
            str: æ ¼å¼åŒ–åçš„çƒ­æœæ–‡æœ¬
        """
        if not news_list:
            return 'æŠ±æ­‰ï¼Œæš‚æ—¶æ— æ³•è·å–ç™¾åº¦çƒ­æœå†…å®¹ã€‚'
        
        result = 'ğŸ“° ç™¾åº¦çƒ­æœæ¦œï¼ˆå‰åæ¡ï¼‰\n\n'
        for news in news_list:
            heat_info = f' [{news["heat"]}]' if news["heat"] else ''
            result += f'{news["rank"]}. **{news["title"]}**{heat_info}\n'
        
        return result.strip()

if __name__ == '__main__':
    # æµ‹è¯•çˆ¬è™«åŠŸèƒ½
    crawler = NewsCrawler()
    news = crawler.fetch_news()
    print(crawler.format_news_response(news))